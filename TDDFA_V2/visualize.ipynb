{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shitianhao/.conda/envs/datproc/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import cv2\n",
    "import yaml\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from FaceBoxes import FaceBoxes\n",
    "from TDDFA import TDDFA\n",
    "from utils.render import render\n",
    "#from utils.render_ctypes import render  # faster\n",
    "from utils.depth import depth\n",
    "from utils.pncc import pncc\n",
    "from utils.uv import uv_tex\n",
    "from utils.pose import calc_pose, P2sRt, matrix2angle\n",
    "from utils.serialization import ser_to_ply, ser_to_obj\n",
    "from utils.functions import draw_landmarks, get_suffix\n",
    "from utils.tddfa_util import str2bool\n",
    "\n",
    "from recrop_images import find_center_bbox, crop_image, eg3dcamparams, crop_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='The demo of still image of 3DDFA_V2')\n",
    "parser.add_argument('-i', '--input_path', type=str, default='examples/test/data.pkl')\n",
    "parser.add_argument('-o', '--output', type=str, default='examples/test/quads.pkl')\n",
    "parser.add_argument('-j', '--output_json', type=str, default='dataset.json')\n",
    "parser.add_argument('-p', '--prefix', type=str, default='')\n",
    "parser.add_argument('--size', type=int, default=1024)\n",
    "parser.add_argument('--out_dir', type=str, default='./crop_samples/img')\n",
    "parser.add_argument('--mode', type=str, default='gpu', help='gpu or cpu mode')\n",
    "parser.add_argument('--config', type=str, default='configs/mb1_120x120.yml')\n",
    "parser.add_argument('--individual', action='store_true', default=False)\n",
    "parser.add_argument('--onnx', action='store_true', default=False)\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = yaml.load(open(args.config), Loader=yaml.SafeLoader) #mb05_120x120\n",
    "\n",
    "# Init FaceBoxes and TDDFA, recommend using onnx flag\n",
    "if args.onnx:\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "    os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "    from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n",
    "    from TDDFA_ONNX import TDDFA_ONNX\n",
    "\n",
    "    face_boxes = FaceBoxes_ONNX()\n",
    "    tddfa = TDDFA_ONNX(**cfg)\n",
    "else:\n",
    "    gpu_mode = args.mode == 'gpu'\n",
    "    tddfa = TDDFA(gpu_mode=gpu_mode, **cfg)\n",
    "    face_boxes = FaceBoxes()\n",
    "\n",
    "\n",
    "with open(args.input_path, \"rb\") as f:\n",
    "    inputs = pickle.load(f, encoding=\"latin1\").items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_bound(lm, method=\"ffhq\"):\n",
    "    if len(lm) == 106:\n",
    "        left_e = lm[104]\n",
    "        right_e = lm[105]\n",
    "        nose = lm[49]\n",
    "        left_m = lm[84]\n",
    "        right_m = lm[90]\n",
    "        center = (lm[1] + lm[31]) * 0.5\n",
    "    elif len(lm) == 68:\n",
    "        left_e = np.mean(lm[36:42], axis=0)\n",
    "        right_e = np.mean(lm[42:48], axis=0)\n",
    "        nose = lm[33]\n",
    "        left_m = lm[48]\n",
    "        right_m = lm[54]\n",
    "        center = (lm[0] + lm[16]) * 0.5\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type of keypoints with a length of {len(lm)}\")\n",
    "\n",
    "\n",
    "\n",
    "    if method == \"ffhq\":\n",
    "        eye_to_eye = right_e - left_e\n",
    "        eye_avg = (left_e + right_e) * 0.5\n",
    "        mouth_avg = (left_m + right_m) * 0.5\n",
    "        eye_to_mouth = mouth_avg - eye_avg\n",
    "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
    "        x /= np.hypot(*x)\n",
    "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
    "        y = np.flipud(x) * [-1, 1]\n",
    "        c = eye_avg + eye_to_mouth * 0.1\n",
    "    elif method == \"default\":\n",
    "        eye_to_eye = right_e - left_e\n",
    "        eye_avg = (left_e + right_e) * 0.5\n",
    "        eye_to_nose = nose - eye_avg\n",
    "        x = eye_to_eye.copy()\n",
    "        x /= np.hypot(*x)\n",
    "        x *= max(np.hypot(*eye_to_eye) * 2.4, np.hypot(*eye_to_nose) * 2.75)\n",
    "        y = np.flipud(x) * [-1, 1]\n",
    "        c = center\n",
    "    else:\n",
    "        raise ValueError('%s crop method not supported yet.' % method)\n",
    "    crop_keypoint_dict = {\n",
    "        \"left_eye\": left_e,\n",
    "        \"right_eye\": right_e,\n",
    "        \"nose\": nose,\n",
    "        \"left_mouth\": left_m,\n",
    "        \"right_mouth\": right_m,\n",
    "        \"center\": center,\n",
    "    }\n",
    "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
    "    quad_dict = {\n",
    "        \"left\": c - x - y,\n",
    "        \"right\": c + x - y,\n",
    "        \"top\": c - x + y,\n",
    "        \"bottom\": c + x + y,\n",
    "    }\n",
    "    return quad.astype(np.float32), c, x, y, crop_keypoint_dict, quad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "size = 512\n",
    "results_quad = {}\n",
    "results_meta = {}\n",
    "for i,item in enumerate(tqdm(inputs)):\n",
    "\n",
    "    # get initial cropping box(quad) using landmarks\n",
    "    img_path, landmarks = item #img_path: str, landmarks: np.ndarray\n",
    "    img_path = args.prefix + img_path\n",
    "    img_orig = cv2.imread(img_path, flags=cv2.IMREAD_COLOR)\n",
    "    if img_orig is None:\n",
    "        print(f'Cannot load image')\n",
    "        continue\n",
    "    quad, quad_c, quad_x, quad_y, kpd, qdd = get_crop_bound(landmarks)\n",
    "\n",
    "    # draw kepoints\n",
    "    from matplotlib import pyplot as plt\n",
    "    for landmark_name, coordinates in qdd.items():\n",
    "        x, y = coordinates\n",
    "        # Draw a point (circle) on the image\n",
    "        cv2.circle(img_orig, (int(x), int(y)), 20, (0, 255, 0), -1)  # You can adjust the color and size here\n",
    "        # Add the landmark name as text near the point\n",
    "        cv2.putText(img_orig, landmark_name, (int(x) + 10, int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 5)\n",
    "\n",
    "    skip = False\n",
    "    for iteration in range(1):\n",
    "        bound = np.array([[0, 0], [0, size-1], [size-1, size-1], [size-1, 0]], dtype=np.float32)\n",
    "        mat = cv2.getAffineTransform(quad[:3], bound[:3])\n",
    "        img = crop_image(img_orig, mat, size, size) # 裁剪后resize成512x512\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # Detect faces, get 3DMM params and roi boxes\n",
    "        boxes = face_boxes(img) # why detect face second time\n",
    "        xmin, ymin, xmax, ymax, score = boxes[0]\n",
    "        color = (0, 255, 0)  # Define the color (BGR format, so (0, 255, 0) is green)\n",
    "        thickness = 2  # Define the thickness of the bounding box\n",
    "        cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, thickness)\n",
    "        if len(boxes) == 0:\n",
    "            print(f'No face detected')\n",
    "            skip = True\n",
    "            break\n",
    "\n",
    "        param_lst, roi_box_lst = tddfa(img, boxes)\n",
    "        box_idx = find_center_bbox(roi_box_lst, w, h)\n",
    "\n",
    "        param = param_lst[box_idx]\n",
    "        P = param[:12].reshape(3, -1)  # camera matrix\n",
    "        s_relative, R, t3d = P2sRt(P)\n",
    "        pose = matrix2angle(R)\n",
    "        pose = [p * 180 / np.pi for p in pose]\n",
    "\n",
    "        # Adjust z-translation in object space\n",
    "        R_ = param[:12].reshape(3, -1)[:, :3]\n",
    "        u = tddfa.bfm.u.reshape(3, -1, order='F')\n",
    "        trans_z = np.array([ 0, 0, 0.5*u[2].mean() ]) # Adjust the object center\n",
    "        trans = np.matmul(R_, trans_z.reshape(3,1))\n",
    "        t3d += trans.reshape(3)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Camera extrinsic estimation for GAN training '''\n",
    "# Normalize P to fit in the original image (before 3DDFA cropping)\n",
    "sx, sy, ex, ey = roi_box_lst[0]\n",
    "scale_x = (ex - sx) / tddfa.size\n",
    "scale_y = (ey - sy) / tddfa.size\n",
    "t3d[0] = (t3d[0]-1) * scale_x + sx\n",
    "t3d[1] = (tddfa.size-t3d[1]) * scale_y + sy\n",
    "t3d[0] = (t3d[0] - 0.5*(w-1)) / (0.5*(w-1)) # Normalize to [-1,1]\n",
    "t3d[1] = (t3d[1] - 0.5*(h-1)) / (0.5*(h-1)) # Normalize to [-1,1], y is flipped for image space\n",
    "t3d[1] *= -1\n",
    "t3d[2] = 0 # orthogonal camera is agnostic to Z (the model always outputs 66.67)\n",
    "\n",
    "s_relative = s_relative * 2000\n",
    "scale_x = (ex - sx) / (w-1)\n",
    "scale_y = (ey - sy) / (h-1)\n",
    "s = (scale_x + scale_y) / 2 * s_relative\n",
    "# print(f\"[{iteration}] s={s} t3d={t3d}\")\n",
    "\n",
    "if s < 0.7 or s > 1.3:\n",
    "    print(f\"Skipping[{i+1-len(results_quad)}/{i+1}]: {img_path} s={s}\")\n",
    "    skip = True\n",
    "if abs(pose[0]) > 90 or abs(pose[1]) > 80 or abs(pose[2]) > 50:\n",
    "    print(f\"Skipping[{i+1-len(results_quad)}/{i+1}]: {img_path} pose={pose}\")\n",
    "    skip = True\n",
    "if abs(t3d[0]) > 1. or abs(t3d[1]) > 1.:\n",
    "    print(f\"Skipping[{i+1-len(results_quad)}/{i+1}]: {img_path} pose={pose} t3d={t3d}\")\n",
    "    skip = True\n",
    "\n",
    "quad_c = quad_c + quad_x * t3d[0]\n",
    "quad_c = quad_c - quad_y * t3d[1]\n",
    "quad_x = quad_x * s\n",
    "quad_y = quad_y * s\n",
    "c, x, y = quad_c, quad_x, quad_y\n",
    "quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y]).astype(np.float32)\n",
    "\n",
    "\n",
    "# final projection matrix\n",
    "s = 1\n",
    "t3d = 0 * t3d\n",
    "R[:,:3] = R[:,:3] * s\n",
    "P = np.concatenate([R,t3d[:,None]],1)\n",
    "P = np.concatenate([P, np.array([[0,0,0,1.]])],0)\n",
    "results_meta[img_path] = eg3dcamparams(P.flatten())\n",
    "results_quad[img_path] = quad\n",
    "\n",
    "# Save cropped images\n",
    "cropped_img = crop_final(img_orig, size=size, quad=quad)\n",
    "os.makedirs(args.out_dir, exist_ok=True)\n",
    "cv2.imwrite(os.path.join(args.out_dir, os.path.basename(img_path).replace(\".png\",\".jpg\")), cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save quads\n",
    "print(\"results:\", len(results_quad))\n",
    "with open(args.output, 'wb') as f:\n",
    "    pickle.dump(results_quad, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save meta data\n",
    "results_new = []\n",
    "for img, P  in results_meta.items():\n",
    "    img = os.path.basename(img)\n",
    "    res = [format(r, '.6f') for r in P]\n",
    "    results_new.append((img,res))\n",
    "with open(os.path.join(args.out_dir, args.output_json), 'w') as outfile:\n",
    "    json.dump({\"labels\": results_new}, outfile, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datproc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
